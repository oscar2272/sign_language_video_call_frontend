import React, { useEffect, useRef, useState } from "react";

const BASE_URL = "http://localhost:8000"; // ì˜ˆì‹œ URL
const CALL_API_URL = `${BASE_URL}/api/calls`;
const WS_BASE_URL = "ws://localhost:8000";
const AI_WS_URL = `${WS_BASE_URL}/ai`;

// MediaPipe íƒ€ì… ì •ì˜
interface MediaPipeHands {
  setOptions: (options: any) => void;
  onResults: (callback: (results: any) => void) => void;
  send: (data: { image: HTMLVideoElement }) => Promise<void>;
}

interface HandLandmark {
  x: number;
  y: number;
}

interface HandData {
  type: string;
  room_id: string;
  landmarks: HandLandmark[][];
  timestamp: number;
}

interface AIResult {
  type: string;
  room_id: string;
  frame_id?: number;
  text: string;
  score?: number;
  timestamp?: number;
}

declare global {
  interface Window {
    Hands: new (config: {
      locateFile: (file: string) => string;
    }) => MediaPipeHands;
  }
}

export default function CallPage() {
  // ê¸°ì¡´ ìƒíƒœë“¤
  const [localStream, setLocalStream] = useState<MediaStream | null>(null);
  const [remoteStream, setRemoteStream] = useState<MediaStream | null>(null);
  const [callStatus, setCallStatus] = useState<
    "calling" | "connecting" | "connected" | "rejected" | "ended"
  >("calling");
  const [isCameraOn, setIsCameraOn] = useState(true);
  const [isMicOn, setIsMicOn] = useState(true);
  const [connectionTime, setConnectionTime] = useState(0);
  const [debugInfo, setDebugInfo] = useState<string[]>([]);

  // AI ê¸°ëŠ¥ ê´€ë ¨ ìƒíƒœ
  const [isAiEnabled, setIsAiEnabled] = useState(false);
  const [isAiConnected, setIsAiConnected] = useState(false);
  const [aiStatus, setAiStatus] = useState<
    "enabled_by_me" | "enabled_by_other" | "disabled"
  >("disabled");
  const [subtitle, setSubtitle] = useState("");
  const [subtitleVisible, setSubtitleVisible] = useState(false);

  // refs
  const localVideoRef = useRef<HTMLVideoElement | null>(null);
  const remoteVideoRef = useRef<HTMLVideoElement | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const aiWsRef = useRef<WebSocket | null>(null);
  const pcRef = useRef<RTCPeerConnection | null>(null);
  const connectionTimeRef = useRef<NodeJS.Timeout | null>(null);
  const pendingCandidatesRef = useRef<RTCIceCandidateInit[]>([]);
  const localStreamRef = useRef<MediaStream | null>(null);

  // MediaPipe ê´€ë ¨ refs
  const handsRef = useRef<MediaPipeHands | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const frameIntervalRef = useRef<NodeJS.Timeout | null>(null);

  const roomId = "test-room-123"; // ì‹¤ì œë¡œëŠ” propsì—ì„œ ë°›ì•„ì˜¬ ê°’
  const user = { id: "user-123" }; // ì‹¤ì œë¡œëŠ” contextì—ì„œ ë°›ì•„ì˜¬ ê°’
  const token = "test-token"; // ì‹¤ì œë¡œëŠ” contextì—ì„œ ë°›ì•„ì˜¬ ê°’

  // ë””ë²„ê·¸ ë¡œê·¸ í•¨ìˆ˜
  const addDebugLog = (message: string) => {
    console.log(`[CallPage] ${message}`);
    setDebugInfo((prev) => [
      ...prev.slice(-4),
      `${new Date().toLocaleTimeString()}: ${message}`,
    ]);
  };

  // MediaPipe ì´ˆê¸°í™”
  const initializeMediaPipe = async (): Promise<MediaPipeHands | null> => {
    try {
      addDebugLog("Initializing MediaPipe Hands...");

      // MediaPipe Hands ë¡œë“œ (CDN ì‚¬ìš©)
      if (typeof window !== "undefined" && !window.Hands) {
        const script = document.createElement("script");
        script.src = "https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js";
        document.head.appendChild(script);

        await new Promise<void>((resolve) => {
          script.onload = () => resolve();
        });
      }

      if (window.Hands) {
        const hands = new window.Hands({
          locateFile: (file: string) => {
            return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
          },
        });

        hands.setOptions({
          maxNumHands: 2,
          modelComplexity: 0, // CPU ëª¨ë“œë¥¼ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
          minDetectionConfidence: 0.5,
          minTrackingConfidence: 0.5,
        });

        hands.onResults((results: any) => {
          if (
            isAiEnabled &&
            results.multiHandLandmarks &&
            results.multiHandLandmarks.length > 0
          ) {
            sendHandLandmarks(results.multiHandLandmarks);
          }
        });

        handsRef.current = hands;
        addDebugLog("MediaPipe Hands initialized successfully");
        return hands;
      }
    } catch (error) {
      addDebugLog(`MediaPipe initialization error: ${error}`);
    }
    return null;
  };

  // ì† ì¢Œí‘œ ë°ì´í„° ì „ì†¡ (15fps)
  const sendHandLandmarks = (landmarks: any[][]) => {
    if (!aiWsRef.current || aiWsRef.current.readyState !== WebSocket.OPEN) {
      return;
    }

    // ì¢Œí‘œ ë°ì´í„° ë³€í™˜
    const landmarksData = landmarks.map((handLandmarks: any[]) =>
      handLandmarks.map((landmark: any) => ({
        x: landmark.x,
        y: landmark.y,
      }))
    );

    const data: HandData = {
      type: "hand_landmarks",
      room_id: roomId,
      landmarks: landmarksData,
      timestamp: Date.now(),
    };

    try {
      aiWsRef.current.send(JSON.stringify(data));
      console.log("Hand landmarks sent:", data); // ë””ë²„ê·¸ìš©
    } catch (error) {
      addDebugLog(`Error sending hand landmarks: ${error}`);
    }
  };

  // AI WebSocket ì—°ê²°
  const connectAiWebSocket = () => {
    if (aiWsRef.current) {
      aiWsRef.current.close();
    }

    addDebugLog("Connecting to AI WebSocket...");
    const aiWs = new WebSocket(`${AI_WS_URL}?role=client&room=${roomId}`);

    aiWs.onopen = () => {
      addDebugLog("AI WebSocket connected");
      setIsAiConnected(true);
    };

    aiWs.onmessage = (event: MessageEvent) => {
      try {
        const data = JSON.parse(event.data);
        addDebugLog(`AI WebSocket message: ${data.type}`);

        switch (data.type) {
          case "ai_result":
            // ìë§‰ í‘œì‹œ
            const result = data as AIResult;
            setSubtitle(result.text || "");
            setSubtitleVisible(true);
            setTimeout(() => setSubtitleVisible(false), 3000); // 3ì´ˆ í›„ ìë§‰ ìˆ¨ê¹€
            addDebugLog(`Received subtitle: ${result.text}`);
            break;

          case "ai_status":
            // AI ìƒíƒœ ì—…ë°ì´íŠ¸ (ë‹¤ë¥¸ ì‚¬ìš©ìê°€ AIë¥¼ ì¼°ì„ ë•Œ)
            if (data.enabled_by !== user.id) {
              setAiStatus(data.enabled ? "enabled_by_other" : "disabled");
            }
            break;
        }
      } catch (error) {
        addDebugLog(`Error parsing AI message: ${error}`);
      }
    };

    aiWs.onclose = () => {
      addDebugLog("AI WebSocket disconnected");
      setIsAiConnected(false);
    };

    aiWs.onerror = (error: Event) => {
      addDebugLog(`AI WebSocket error: ${error}`);
    };

    aiWsRef.current = aiWs;
  };

  // AI ê¸°ëŠ¥ í† ê¸€
  const toggleAiFeature = () => {
    if (aiStatus === "enabled_by_other") {
      addDebugLog("Cannot toggle AI - enabled by other user");
      return;
    }

    const newEnabled = !isAiEnabled;
    setIsAiEnabled(newEnabled);
    setAiStatus(newEnabled ? "enabled_by_me" : "disabled");

    // AI ìƒíƒœë¥¼ ë‹¤ë¥¸ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
    if (aiWsRef.current && aiWsRef.current.readyState === WebSocket.OPEN) {
      aiWsRef.current.send(
        JSON.stringify({
          type: "ai_status",
          room_id: roomId,
          enabled: newEnabled,
          enabled_by: user.id,
        })
      );
    }

    if (newEnabled) {
      startHandTracking();
      addDebugLog("AI feature enabled - hand tracking started");
    } else {
      stopHandTracking();
      addDebugLog("AI feature disabled - hand tracking stopped");
    }
  };

  // ì† ì¶”ì  ì‹œì‘ (15fps)
  const startHandTracking = async () => {
    if (!handsRef.current || !localVideoRef.current) {
      addDebugLog("MediaPipe or video not ready for hand tracking");
      return;
    }

    // 15fps = 66.67ms ê°„ê²©
    frameIntervalRef.current = setInterval(async () => {
      if (localVideoRef.current && handsRef.current && isAiEnabled) {
        try {
          await handsRef.current.send({ image: localVideoRef.current });
        } catch (error) {
          addDebugLog(`Hand tracking error: ${error}`);
        }
      }
    }, 1000 / 15) as NodeJS.Timeout; // 15fps

    addDebugLog("Hand tracking started at 15fps");
  };

  // ì† ì¶”ì  ì¤‘ì§€
  const stopHandTracking = () => {
    if (frameIntervalRef.current) {
      clearInterval(frameIntervalRef.current);
      frameIntervalRef.current = null;
    }
    addDebugLog("Hand tracking stopped");
  };

  // WebRTC ì„¤ì • (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
  const createPeerConnection = () => {
    addDebugLog("Creating peer connection...");

    const pc = new RTCPeerConnection({
      iceServers: [
        { urls: "stun:stun.l.google.com:19302" },
        { urls: "stun:stun1.l.google.com:19302" },
      ],
    });

    pc.onicecandidate = (event) => {
      if (event.candidate && wsRef.current?.readyState === WebSocket.OPEN) {
        addDebugLog("Sending ICE candidate");
        wsRef.current.send(
          JSON.stringify({
            type: "ice",
            candidate: event.candidate,
          })
        );
      }
    };

    pc.ontrack = (event) => {
      addDebugLog("Remote track received");
      setRemoteStream(event.streams[0]);
    };

    pc.onconnectionstatechange = () => {
      const state = pc.connectionState;
      addDebugLog(`Connection state changed: ${state}`);
      if (state === "connected") {
        setCallStatus("connected");
        startConnectionTimer();
      }
    };

    return pc;
  };

  // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì´ˆê¸°í™”
  const initializeMedia = async (): Promise<MediaStream | null> => {
    try {
      addDebugLog("Requesting media access...");
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: 1280, height: 720 },
        audio: true,
      });

      localStreamRef.current = stream;
      setLocalStream(stream);
      addDebugLog("Media access granted");

      if (localVideoRef.current) {
        localVideoRef.current.srcObject = stream;
      }

      return stream;
    } catch (error) {
      addDebugLog(`Media access error: ${error}`);
      alert("ì¹´ë©”ë¼ì™€ ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.");
      return null;
    }
  };

  // WebSocket ì—°ê²°
  const connectWebSocket = (stream: MediaStream): WebSocket => {
    addDebugLog("Connecting to WebSocket...");
    const ws = new WebSocket(
      `${WS_BASE_URL}/ws/call/${roomId}/?user_id=${user.id}`
    );

    ws.onopen = () => {
      addDebugLog("WebSocket connected");
      setCallStatus("connecting");
    };

    ws.onmessage = async (event: MessageEvent) => {
      const data = JSON.parse(event.data);
      addDebugLog(`WebSocket message: ${data.type}`);
      // WebRTC ì‹œê·¸ë„ë§ ë¡œì§...
    };

    return ws;
  };

  // ì—°ê²° ì‹œê°„ íƒ€ì´ë¨¸
  const startConnectionTimer = () => {
    connectionTimeRef.current = setInterval(() => {
      setConnectionTime((prev) => prev + 1);
    }, 1000) as NodeJS.Timeout;
  };

  // ì‹œê°„ í¬ë§·íŒ…
  const formatTime = (seconds: number): string => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}`;
  };

  // ì¹´ë©”ë¼/ë§ˆì´í¬ í† ê¸€
  const toggleCamera = () => {
    const stream = localStreamRef.current;
    if (stream) {
      const videoTrack = stream.getVideoTracks()[0];
      if (videoTrack) {
        videoTrack.enabled = !videoTrack.enabled;
        setIsCameraOn(videoTrack.enabled);
      }
    }
  };

  const toggleMic = () => {
    const stream = localStreamRef.current;
    if (stream) {
      const audioTrack = stream.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !audioTrack.enabled;
        setIsMicOn(audioTrack.enabled);
      }
    }
  };

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = () => {
    addDebugLog("Cleaning up resources...");

    if (connectionTimeRef.current) {
      clearInterval(connectionTimeRef.current);
    }

    stopHandTracking();

    if (localStreamRef.current) {
      localStreamRef.current
        .getTracks()
        .forEach((track: MediaStreamTrack) => track.stop());
    }

    if (aiWsRef.current) {
      aiWsRef.current.close();
    }

    if (wsRef.current) {
      wsRef.current.close();
    }
  };

  // ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
  useEffect(() => {
    const init = async () => {
      // ë¯¸ë””ì–´ ë° AI WebSocket ì´ˆê¸°í™”
      const stream = await initializeMedia();
      if (!stream) return;

      await initializeMediaPipe();
      connectAiWebSocket();
      wsRef.current = connectWebSocket(stream);
    };

    init();
    return cleanup;
  }, []);

  // ì›ê²© ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì„¤ì •
  useEffect(() => {
    if (remoteStream && remoteVideoRef.current) {
      remoteVideoRef.current.srcObject = remoteStream;
    }
  }, [remoteStream]);

  return (
    <div className="fixed inset-0 bg-gray-900 flex flex-col h-screen">
      {/* ìƒíƒœ í‘œì‹œ */}
      <div className="bg-gray-800 text-white p-3 text-center flex-shrink-0 min-h-[60px] flex items-center justify-between">
        <div className="flex-1">
          {callStatus === "calling" && <span>ì „í™” ê±°ëŠ” ì¤‘...</span>}
          {callStatus === "connecting" && <span>ì—°ê²° ì¤‘...</span>}
          {callStatus === "connected" && (
            <span>í†µí™” ì¤‘ - {formatTime(connectionTime)}</span>
          )}
        </div>

        {/* AI ìƒíƒœ í‘œì‹œ */}
        <div className="text-sm">
          {isAiConnected && <span className="text-green-400">AI ì—°ê²°ë¨</span>}
          {aiStatus === "enabled_by_me" && (
            <span className="text-blue-400 ml-2">AI í™œì„±í™”</span>
          )}
          {aiStatus === "enabled_by_other" && (
            <span className="text-yellow-400 ml-2">ìƒëŒ€ë°©ì´ AI ì‚¬ìš© ì¤‘</span>
          )}
        </div>
      </div>

      {/* ë””ë²„ê·¸ ì •ë³´ */}
      <div className="bg-red-900 text-white p-2 text-xs flex-shrink-0 max-h-20 overflow-y-auto">
        {debugInfo.map((info, index) => (
          <div key={index}>{info}</div>
        ))}
      </div>

      {/* ë¹„ë””ì˜¤ ì˜ì—­ */}
      <div
        className="flex-1 relative min-h-0"
        style={{ maxHeight: "calc(100vh - 160px)" }}
      >
        {/* ì›ê²© ë¹„ë””ì˜¤ */}
        <video
          ref={remoteVideoRef}
          autoPlay
          playsInline
          className="w-full h-full object-cover"
          style={{ transform: "scaleX(-1)" }}
        />

        {/* ë¡œì»¬ ë¹„ë””ì˜¤ */}
        <video
          ref={localVideoRef}
          autoPlay
          playsInline
          muted
          className="absolute top-2 right-2 w-32 h-24 bg-gray-800 rounded-lg object-cover border-2 border-white"
          style={{ transform: "scaleX(-1)" }}
        />

        {/* ìë§‰ í‘œì‹œ */}
        {subtitleVisible && subtitle && (
          <div className="absolute bottom-4 left-1/2 transform -translate-x-1/2 bg-black bg-opacity-70 text-white px-4 py-2 rounded-lg text-lg font-semibold max-w-md text-center">
            {subtitle}
          </div>
        )}

        {/* ì—°ê²° ëŒ€ê¸° í”Œë ˆì´ìŠ¤í™€ë” */}
        {!remoteStream && callStatus !== "ended" && (
          <div className="absolute inset-0 flex items-center justify-center text-white text-xl">
            ìƒëŒ€ë°©ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...
          </div>
        )}
      </div>

      {/* ì»¨íŠ¸ë¡¤ ë²„íŠ¼ */}
      <div className="bg-gray-800 flex-shrink-0 p-4 min-h-[100px]">
        <div className="flex justify-center gap-4 mb-2">
          <button
            onClick={toggleMic}
            className={`px-4 py-2 rounded ${
              isMicOn
                ? "bg-blue-600 hover:bg-blue-700"
                : "bg-red-600 hover:bg-red-700"
            } text-white text-sm`}
          >
            {isMicOn ? "ğŸ¤ ë§ˆì´í¬ ì¼œì§" : "ğŸ”‡ ë§ˆì´í¬ êº¼ì§"}
          </button>

          <button
            onClick={toggleCamera}
            className={`px-4 py-2 rounded ${
              isCameraOn
                ? "bg-blue-600 hover:bg-blue-700"
                : "bg-red-600 hover:bg-red-700"
            } text-white text-sm`}
          >
            {isCameraOn ? "ğŸ“¹ ì¹´ë©”ë¼ ì¼œì§" : "ğŸ“· ì¹´ë©”ë¼ êº¼ì§"}
          </button>

          <button
            onClick={() => {
              setCallStatus("ended");
              cleanup();
            }}
            className="px-4 py-2 rounded bg-red-600 hover:bg-red-700 text-white text-sm"
          >
            ğŸ“ í†µí™” ì¢…ë£Œ
          </button>
        </div>

        {/* AI ê¸°ëŠ¥ í† ê¸€ */}
        <div className="flex justify-center">
          <button
            onClick={toggleAiFeature}
            disabled={aiStatus === "enabled_by_other"}
            className={`px-6 py-2 rounded text-sm font-semibold ${
              aiStatus === "enabled_by_other"
                ? "bg-gray-600 cursor-not-allowed text-gray-400"
                : isAiEnabled
                  ? "bg-green-600 hover:bg-green-700 text-white"
                  : "bg-purple-600 hover:bg-purple-700 text-white"
            }`}
          >
            {aiStatus === "enabled_by_other"
              ? "ğŸ¤– ìƒëŒ€ë°©ì´ AI ì‚¬ìš© ì¤‘"
              : isAiEnabled
                ? "ğŸ¤– AI ë²ˆì—­ ì¤‘ì§€"
                : "ğŸ¤– AI ë²ˆì—­ ì‹œì‘"}
          </button>
        </div>
      </div>

      {/* ìˆ¨ê²¨ì§„ ìº”ë²„ìŠ¤ (MediaPipe ì²˜ë¦¬ìš©) */}
      <canvas ref={canvasRef} className="hidden" />
    </div>
  );
}
